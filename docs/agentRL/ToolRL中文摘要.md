# ToolRL: 奖励是工具学习所需的全部

**作者：** Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, Heng Ji  
**机构：** 伊利诺伊大学厄巴纳-香槟分校  
**联系方式：** {chengq9, hengji}@illinois.edu

## 摘要

当前的大型语言模型（LLMs）通常通过监督微调（SFT）来获取工具使用能力。然而，SFT在面对不熟悉或复杂的工具使用场景时，泛化能力有限。强化学习（RL）领域的最新进展，特别是类似R1的模型，已经展示出令人瞩目的推理和泛化能力。但是，工具使用的奖励设计面临独特挑战：可能需要调用多种工具，每种工具都有不同的参数，而粗粒度的奖励信号（如答案匹配）无法提供有效学习所需的精细反馈。

在本研究中，我们对RL范式下的工具选择和应用任务的奖励设计进行了首次全面研究。我们系统地探索了广泛的奖励策略，分析了它们的类型、规模、粒度和时间动态。基于这些见解，我们提出了一个专为工具使用任务量身定制的原则性奖励设计，并将其应用于使用群组相对策略优化（GRPO）训练LLMs。在多个基准测试中的实证评估表明，我们的方法产生了稳健、可扩展和稳定的训练，比基础模型提高了17%，比SFT模型提高了15%。这些结果突显了精心设计奖励在增强LLMs工具使用能力和泛化性能方面的关键作用。

## 工具集成推理的重要性

工具集成推理（Tool-Integrated Reasoning，TIR）涉及LLMs与外部工具的交互，如搜索引擎、计算器或代码解释器，通过多步骤、反馈驱动的循环来获得解决方案。

TIR特别重要，因为它解决了LLMs的核心限制，如过时知识、计算不准确和浅层推理。通过集成提供实时访问和专业能力的外部工具，TIR使模型能够以更加扎实和目标导向的方式处理复杂任务。

## SFT的局限性

监督微调（SFT）虽然在一定程度上有效，但在泛化、探索和适应性方面存在困难。如图1所示，在深度思考轨迹上训练的SFT模型会过度解释工具，无法拒绝不适当的工具，仅仅模仿"但等等"等提示，而没有进行真正的深度思考。因此，SFT通常无法捕捉到最佳工具使用所需的策略灵活性，特别是在开放式或多步骤设置中。

## 奖励策略的关键维度

为了确定最佳奖励策略，我们系统地探索了广泛的奖励配置，涵盖四个关键维度：

1. **奖励类型**（奖励什么方面）
2. **奖励规模**（奖励多少）
3. **奖励粒度**（奖励信号的详细程度）
4. **奖励动态**（奖励如何随时间演变）

通过大量实验，我们确定了最适合代理工具使用的奖励设计，并揭示了什么使奖励对调用工具的LLMs"有用"的见解。

### 核心见解

* **更长的推理轨迹并非本质上更好**，长度奖励可能会降低性能
* **动态奖励规模**有助于模型从简单到复杂行为的平稳过渡
* **细粒度奖励分解**导致更稳定和有效的学习

## 奖励设计详解

基于规则的奖励机制已经证明了强大的实证性能，并被广泛采用。在我们的训练中，我们采用了结合结构和正确性的奖励公式。

总体奖励Rfinal(·)分解为两个组成部分：Rformat + Rcorrect

### 格式奖励

格式奖励Rformat ∈ {0, 1}检查模型输出是否包含按照真实标准指定的正确顺序的所有必需特殊标记：

Rformat = {
  1, 如果所有必需字段出现且顺序正确
  0, 否则
}

### 正确性奖励

正确性奖励Rcorrect ∈ [−3, 3]评估预测的工具调用P = {P1, ..., Pm}与真实调用G = {G1, ..., Gn}的匹配程度。它包括三个组成部分：

#### 1. 工具名称匹配

rname = |NG ∩ NP| / |NG ∪ NP| ∈ [0, 1]

其中NG和NP分别是从真实和预测工具调用中提取的工具名称集合。

#### 2. 参数名称匹配

rparam = ∑(Gj∈G) |keys(PG) ∩ keys(PP)| / |keys(PG) ∪ keys(PP)| ∈ [0, |G|]

其中keys(PG)和keys(PP)分别表示预测和真实工具调用的参数名称。

#### 3. 参数内容匹配

rvalue = ∑(Gj∈G) ∑(k∈keys(Gj)) 1[PG[k] = PP[k]] ∈ [0, ∑(Gj∈G) |keys(Gj)|]

其中PG[k]和PP[k]分别表示预测和真实工具调用的参数值。

#### 4. 每个匹配的总分数

rmatch = rname + rparam + rvalue ∈ [0, Smax]

其中Smax = 1 + |G| + ∑(Gj∈G) |keys(Gj)| 表示最大可能分数。

总分通过找到P和G之间的最佳匹配来计算，以最大化总匹配分数：

Rcorrect = 6 · (Rmax/Smax) - 3 ∈ [-3, 3]

其中Rmax表示来自最佳匹配的总匹配分数。

### 最终奖励计算

最终奖励值Rfinal是Rformat和Rcorrect的总和：

Rfinal = Rformat + Rcorrect ∈ [-3, 4]

## 奖励设计的优势

1. **结构与语义分离**：我们的奖励设计通过明确分离结构合规性和语义正确性，确保了平衡且可解释的评估信号

2. **细粒度评估**：通过评估工具名称、参数名称和参数值的匹配程度，提供了更精细的反馈

3. **归一化处理**：将奖励值标准化到特定范围内，确保训练稳定性

4. **组合奖励**：结合格式和正确性奖励，引导模型产生不仅在语法上有效，而且在语义上忠实的输出

## 实验结果

我们的方法在多个基准测试中取得了显著成果：

1. 比基础模型提高了17%的性能
2. 比SFT模型提高了15%的性能
3. 展示了对未见场景和任务目标的强泛化能力
4. 表现出主动性和元认知推理等涌现行为

## 结论

本研究开创了将RL应用于通用TIR的先河，并为TIR中的奖励设计提供了第一个实证路线图，为更有能力和自主的LLM代理铺平了道路。

数据和代码发布在 https://github.com/qiancheng0/ToolRL
